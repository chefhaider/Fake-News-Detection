{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.6"
    },
    "colab": {
      "name": "Copy of notebook.ipynb",
      "provenance": [],
      "include_colab_link": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chefhaider/Fake-News-Detection/blob/main/notebook.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "OBMqef4lC22f",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "91de41d4-e02b-4136-c7d9-bc814e2eeaae"
      },
      "source": [
        "pip install pyspellchecker"
      ],
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pyspellchecker in /usr/local/lib/python3.7/dist-packages (0.6.2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GoyZamMDeB5F"
      },
      "source": [
        "import re\n",
        "import string\n",
        "\n",
        "\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import requests\n",
        "\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "\n",
        "from sklearn.metrics import classification_report\n",
        "from sklearn.metrics import confusion_matrix\n",
        "from sklearn import metrics\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "from sklearn.naive_bayes import BernoulliNB\n",
        "from sklearn.feature_extraction.text import CountVectorizer\n",
        "\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.naive_bayes import MultinomialNB\n",
        "\n",
        "from sklearn.pipeline import Pipeline\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "\n",
        "from sklearn.multiclass import OneVsRestClassifier\n",
        "from spellchecker import SpellChecker\n",
        "\n",
        "\n",
        "from keras import Sequential\n",
        "from keras.layers import Dense\n",
        "\n"
      ],
      "execution_count": 68,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZxcqmBkZC1Z8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31ae154b-9e72-4c2a-9bb3-c83c74054fef"
      },
      "source": [
        "nltk.download('stopwords')"
      ],
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 69
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xRJ8fTIId-Pc"
      },
      "source": [
        "df = pd.read_csv('https://raw.githubusercontent.com/chefhaider/Fake-News-Detection-Kaggle/main/train.csv')"
      ],
      "execution_count": 70,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 204
        },
        "id": "080gZcIUdwr3",
        "outputId": "9ef07beb-4407-45ee-b15c-59d11a7c2043"
      },
      "source": [
        "df.head()"
      ],
      "execution_count": 71,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>id</th>\n",
              "      <th>keyword</th>\n",
              "      <th>location</th>\n",
              "      <th>text</th>\n",
              "      <th>target</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>1</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Our Deeds are the Reason of this #earthquake M...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>4</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Forest fire near La Ronge Sask. Canada</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>5</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>All residents asked to 'shelter in place' are ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>6</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>13,000 people receive #wildfires evacuation or...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>7</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>Just got sent this photo from Ruby #Alaska as ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   id keyword  ...                                               text target\n",
              "0   1     NaN  ...  Our Deeds are the Reason of this #earthquake M...      1\n",
              "1   4     NaN  ...             Forest fire near La Ronge Sask. Canada      1\n",
              "2   5     NaN  ...  All residents asked to 'shelter in place' are ...      1\n",
              "3   6     NaN  ...  13,000 people receive #wildfires evacuation or...      1\n",
              "4   7     NaN  ...  Just got sent this photo from Ruby #Alaska as ...      1\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 71
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5ZYL2wPeeGLj",
        "outputId": "4ecf72d0-69bb-489e-d127-aa759d82ab80"
      },
      "source": [
        "df['target'].value_counts()"
      ],
      "execution_count": 72,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "0    4342\n",
              "1    3271\n",
              "Name: target, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 72
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sDsDh_8ed8KN",
        "outputId": "d4c1315e-6029-4f01-aea3-78b7d13b2c6c"
      },
      "source": [
        "df.isnull().sum()"
      ],
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "id             0\n",
              "keyword       61\n",
              "location    2533\n",
              "text           0\n",
              "target         0\n",
              "dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 73
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F7OcT_TH-GbL",
        "outputId": "5fb296ba-505e-42af-ad14-a60f164e4061"
      },
      "source": [
        "len(df.location.unique())"
      ],
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "3342"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 74
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wMMdBdzOokUK",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "b9dae627-6213-4780-e80d-f87b05264d9f"
      },
      "source": [
        "'''import pandas as pd\n",
        "from geopy.geocoders import Nominatim\n",
        "from geopy.exc import GeocoderTimedOut\n",
        "\n",
        "\n",
        "df = pd.read_csv('https://raw.githubusercontent.com/chefhaider/Fake-News-Detection-Kaggle/main/train.csv')\n",
        "\n",
        "\n",
        "geolocator = Nominatim(user_agent = \"geoapiExercises\")\n",
        "city_name = []\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "for val in df.location:\n",
        "  if val == 'nan':\n",
        "    city_name+=[' ']\n",
        "    continue\n",
        "\n",
        "  try:\n",
        "    city_name+=[ geolocator.geocode(val)[-2].split(',')[-1] ]\n",
        "  except TypeError:\n",
        "    city_name+=[' ']\n",
        "  except GeocoderTimedOut :\n",
        "    city_name+=[' ']'''"
      ],
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'import pandas as pd\\nfrom geopy.geocoders import Nominatim\\nfrom geopy.exc import GeocoderTimedOut\\n\\n\\ndf = pd.read_csv(\\'https://raw.githubusercontent.com/chefhaider/Fake-News-Detection-Kaggle/main/train.csv\\')\\n\\n\\ngeolocator = Nominatim(user_agent = \"geoapiExercises\")\\ncity_name = []\\n\\n\\n\\n\\nfor val in df.location:\\n  if val == \\'nan\\':\\n    city_name+=[\\' \\']\\n    continue\\n\\n  try:\\n    city_name+=[ geolocator.geocode(val)[-2].split(\\',\\')[-1] ]\\n  except TypeError:\\n    city_name+=[\\' \\']\\n  except GeocoderTimedOut :\\n    city_name+=[\\' \\']'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 75
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "SvMl5rsFBN6-"
      },
      "source": [
        "#fig = plt.figure(figsize=(10,80))\n",
        "#sns.countplot(y=\"keyword\",order=df[df.target == 1]['keyword'].value_counts().index, hue=\"target\",data = df,palette='Paired')"
      ],
      "execution_count": 76,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NiYN0sWgiwmN"
      },
      "source": [
        "df.dropna(subset = [\"keyword\"], inplace=True)\n",
        "#df.reset_index(drop=True, inplace=True)"
      ],
      "execution_count": 77,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "81jcLnIUN0dU"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "def clean_text(text):\n",
        "\n",
        "\n",
        "    url = re.compile(r'https?://\\S+|www\\.\\S+')\n",
        "    text = url.sub(r'',text)\n",
        "\n",
        "\n",
        "    html=re.compile(r'<.*?>')\n",
        "    text = html.sub(r'',text)\n",
        "\n",
        "\n",
        "\n",
        "    emoji_pattern = re.compile(\"[\"\n",
        "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
        "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
        "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
        "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
        "                           u\"\\U00002702-\\U000027B0\"\n",
        "                           u\"\\U000024C2-\\U0001F251\"\n",
        "                           \"]+\", flags=re.UNICODE)\n",
        "    text = emoji_pattern.sub(r'', text)\n",
        "\n",
        "\n",
        "    table=str.maketrans('','',string.punctuation)\n",
        "    return text.translate(table)\n",
        "\n",
        "\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: clean_text(x))\n"
      ],
      "execution_count": 78,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5fLfUYQHi0mt",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "12817b40-61d5-4c94-eeed-9af71316371d"
      },
      "source": [
        "'''\n",
        "spell = SpellChecker()\n",
        "\n",
        "def correct_spellings(text):\n",
        "    corrected_text = []\n",
        "    misspelled_words = spell.unknown(text.split())\n",
        "    for word in text.split():\n",
        "        if word in misspelled_words:\n",
        "            corrected_text.append(spell.correction(word))\n",
        "        else:\n",
        "            corrected_text.append(word)\n",
        "    return \" \".join(corrected_text)\n",
        "\n",
        "df['text'] = df['text'].apply(lambda x: correct_spellings(x))'''"
      ],
      "execution_count": 79,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            },
            "text/plain": [
              "'\\nspell = SpellChecker()\\n\\ndef correct_spellings(text):\\n    corrected_text = []\\n    misspelled_words = spell.unknown(text.split())\\n    for word in text.split():\\n        if word in misspelled_words:\\n            corrected_text.append(spell.correction(word))\\n        else:\\n            corrected_text.append(word)\\n    return \" \".join(corrected_text)\\n\\ndf[\\'text\\'] = df[\\'text\\'].apply(lambda x: correct_spellings(x))'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 79
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sM4mciC2TPtJ"
      },
      "source": [
        "X_train,X_test,y_train,y_test = train_test_split(df[['text','keyword']], df['target'], test_size=0.4, random_state=42)"
      ],
      "execution_count": 80,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QNLcrmxBHylT",
        "outputId": "086b85f9-5066-4c9e-c45f-43fcc32870ba"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('tfidf', TfidfVectorizer(stop_words=stopwords.words('english'))),\n",
        "    ('clf', OneVsRestClassifier(MultinomialNB(\n",
        "        fit_prior=True, class_prior=None))),\n",
        "])\n",
        "parameters = {\n",
        "    'tfidf__max_df': (0.25, 0.5, 0.75),\n",
        "    'tfidf__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
        "    'clf__estimator__alpha': (1e-2, 1e-3)\n",
        "}\n",
        "\n",
        "m_model = GridSearchCV(pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n",
        "m_model.fit(X_train['text'].values, y_train)\n",
        "\n",
        "print(\"Best parameters set:\")\n",
        "print( m_model.best_estimator_.steps )\n",
        "\n",
        "\n",
        "pred = m_model.predict(X_test['text'])\n",
        "print(classification_report(y_test, pred ))\n",
        "\n"
      ],
      "execution_count": 81,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 18 candidates, totalling 36 fits\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done  28 tasks      | elapsed:    5.2s\n",
            "[Parallel(n_jobs=2)]: Done  36 out of  36 | elapsed:    6.5s finished\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Best parameters set:\n",
            "[('tfidf', TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.float64'>, encoding='utf-8',\n",
            "                input='content', lowercase=True, max_df=0.25, max_features=None,\n",
            "                min_df=1, ngram_range=(1, 3), norm='l2', preprocessor=None,\n",
            "                smooth_idf=True,\n",
            "                stop_words=['i', 'me', 'my', 'myself', 'we', 'our', 'ours',\n",
            "                            'ourselves', 'you', \"you're\", \"you've\", \"you'll\",\n",
            "                            \"you'd\", 'your', 'yours', 'yourself', 'yourselves',\n",
            "                            'he', 'him', 'his', 'himself', 'she', \"she's\",\n",
            "                            'her', 'hers', 'herself', 'it', \"it's\", 'its',\n",
            "                            'itself', ...],\n",
            "                strip_accents=None, sublinear_tf=False,\n",
            "                token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
            "                vocabulary=None)), ('clf', OneVsRestClassifier(estimator=MultinomialNB(alpha=0.01, class_prior=None,\n",
            "                                            fit_prior=True),\n",
            "                    n_jobs=None))]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.83      0.81      1733\n",
            "           1       0.75      0.71      0.73      1288\n",
            "\n",
            "    accuracy                           0.78      3021\n",
            "   macro avg       0.77      0.77      0.77      3021\n",
            "weighted avg       0.78      0.78      0.78      3021\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "49bBgGQgR2uI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6c6d2e46-65dc-49fe-9446-1182f6d2e67c"
      },
      "source": [
        "\n",
        "\n",
        "\n",
        "pipeline = Pipeline([\n",
        "    ('cv', CountVectorizer()),\n",
        "    ('clf', OneVsRestClassifier(BernoulliNB(\n",
        "        fit_prior=True, class_prior=None))),\n",
        "])\n",
        "parameters = {\n",
        "    'clf__estimator__alpha': (1e-2, 1e-3)\n",
        "}\n",
        "\n",
        "b_model = GridSearchCV(pipeline, parameters, cv=2, n_jobs=2, verbose=3)\n",
        "b_model.fit(X_train['keyword'].values, y_train)\n",
        "\n",
        "print(\"Best parameters set:\")\n",
        "print( b_model.best_estimator_.steps )\n",
        "\n",
        "\n",
        "pred = b_model.predict(X_test['keyword'])\n",
        "print(classification_report(y_test, pred ))\n",
        "\n"
      ],
      "execution_count": 82,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fitting 2 folds for each of 2 candidates, totalling 4 fits\n",
            "Best parameters set:\n",
            "[('cv', CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
            "                dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
            "                lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
            "                ngram_range=(1, 1), preprocessor=None, stop_words=None,\n",
            "                strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
            "                tokenizer=None, vocabulary=None)), ('clf', OneVsRestClassifier(estimator=BernoulliNB(alpha=0.01, binarize=0.0,\n",
            "                                          class_prior=None, fit_prior=True),\n",
            "                    n_jobs=None))]\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.76      0.77      0.77      1733\n",
            "           1       0.69      0.67      0.68      1288\n",
            "\n",
            "    accuracy                           0.73      3021\n",
            "   macro avg       0.72      0.72      0.72      3021\n",
            "weighted avg       0.73      0.73      0.73      3021\n",
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "[Parallel(n_jobs=2)]: Using backend LokyBackend with 2 concurrent workers.\n",
            "[Parallel(n_jobs=2)]: Done   4 out of   4 | elapsed:    0.1s finished\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-wSpv7xWT8AO"
      },
      "source": [
        "![alt text]('https://i.ibb.co/km70Wk6/IMG-20210603-142542.jpg')*italicized text*"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8_OgfSETI4V5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4e9ed84e-515a-4996-b786-f1ea73bc0f24"
      },
      "source": [
        "def naiveBayesClassifier(df,prob_text,prob_keyword):\n",
        "\n",
        "  prob_1 = df.value_counts()[1]/len(y_train)\n",
        "\n",
        "  return [ int( ( ( val1[1] * val0[1] ) / prob_1 ) > .5 ) for val1,val0 in zip(prob_text,prob_keyword) ]\n",
        "\n",
        "joint_prob  =   naiveBayesClassifier(y_train,m_model.predict_proba(X_test['text']),b_model.predict_proba(X_test['keyword']))\n",
        "print(classification_report(y_test, joint_prob))"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.79      0.86      0.82      1733\n",
            "           1       0.78      0.69      0.74      1288\n",
            "\n",
            "    accuracy                           0.79      3021\n",
            "   macro avg       0.79      0.78      0.78      3021\n",
            "weighted avg       0.79      0.79      0.79      3021\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BLaE8kcGXDu-"
      },
      "source": [
        "text_prob = [ val[1] for val in m_model.predict_proba(X_test['text']) ]\n",
        "keyword_prob = [ val[1] for val in  b_model.predict_proba(X_test['keyword']) ]\n",
        "\n",
        "prob_df = pd.DataFrame()\n",
        "prob_df['text'] = text_prob\n",
        "prob_df['keyword'] = keyword_prob\n",
        "\n",
        "sns.scatterplot(data=prob_df, x=\"text\", y=\"keyword\")"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3ayKrGEmdMGe"
      },
      "source": [
        "from sklearn.cluster import KMeans\n",
        "\n",
        "kmeans = KMeans(n_clusters=2, init='k-means++', max_iter=300, n_init=10, random_state=0 )\n",
        "predicts = kmeans.fit_predict(prob_df)\n",
        "\n",
        "print(classification_report(y_test, predicts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d4HM0y5mlNxV"
      },
      "source": [
        "prob_df"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ou5eFa3LZGVQ"
      },
      "source": [
        "target = np.array(y_test)\n",
        "\n",
        "\n",
        "X_train,X_test,y_train,y_test = train_test_split(prob_df[['keyword','text']],target, test_size=0.3, random_state=42)\n",
        "\n",
        "classifier = Sequential()\n",
        "#First Hidden Layer\n",
        "#classifier.add(Dense(4, activation='relu', kernel_initializer='random_normal', input_dim=2))\n",
        "#Second  Hidden Layer\n",
        "classifier.add(Dense(4, activation='relu', kernel_initializer='random_normal'))\n",
        "#Output Layer\n",
        "classifier.add(Dense(1, activation='sigmoid', kernel_initializer='random_normal'))\n",
        "\n",
        "\n",
        "#Compiling the neural network\n",
        "classifier.compile(optimizer ='adam',loss='binary_crossentropy', metrics =['accuracy'])\n",
        "\n",
        "#Fitting the data to the training dataset\n",
        "classifier.fit(X_train,y_train, batch_size=10, epochs=100)\n",
        "\n",
        "\n",
        "eval_model=classifier.evaluate(X_train, y_train)\n",
        "\n",
        "predicts = classifier.predict(X_test)\n",
        "predicts =(predicts>0.5)\n",
        "print(classification_report(y_test, predicts))"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TePZGHZUf7Vk"
      },
      "source": [
        "cf_matrix = confusion_matrix(y_test, predicts)\n",
        "labels = ['True Neg','False Pos','False Neg','True Pos']\n",
        "labels = np.asarray(labels).reshape(2,2)\n",
        "sns.heatmap(cf_matrix, annot=labels, fmt='', cmap='Blues')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a0DJmI5NTOV8"
      },
      "source": [
        "'''test = pd.read_csv('https://raw.githubusercontent.com/chefhaider/Fake-News-Detection-Kaggle/main/test.csv')\n",
        "\n",
        "\n",
        "\n",
        "test['text'] = test['text'].apply(lambda x: clean_text(x))\n",
        "tf_idf_mat_test = tf_idf_vect.transform(test['text'].values) # fit_transform vectorizer to dtest['text']\n",
        "predictions = clf.predict(tf_idf_mat_test)\n",
        "\n",
        "output = pd.DataFrame({'id': test.id, 'target': predictions})\n",
        "output.to_csv('my_submission.csv', index=False)'''"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}